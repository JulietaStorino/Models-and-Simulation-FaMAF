{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 9: Cadenas de Markov \n",
    "\n",
    "**Proceso estocástico**: Dado un espacio de probabilidad $(\\Omega, \\mathbb{P})$, un proceso estocástico $X$ en $\\Omega$ es una función $X: T\\times\\Omega \\to \\mathbb{R}$. Así, para cada $t\\in T$, $X(T,\\dot)$ es una variable aleatoria en $\\Omega$, y para cada $\\omega\\in\\Omega$, $X(\\dot,\\omega)$ es una función en $T$.\n",
    "* Proceso estocástico discreto: T es un subconjunto de un conjunto discreto.\n",
    "* Proceso estocástico continuo: T es un subconjunto de un conjunto continuo.\n",
    "\n",
    "**Conjunto de estados**: Está dado por todos los valores que puede tomar el proceso. Esto es $S=\\{X(t,\\omega)|t\\in I,\\quad \\omega\\in\\Omega\\}$.\n",
    "\n",
    "**Cadena**: Es un proceso estocástico donde $T$ es discreto y $S$ es discreto.\n",
    "\n",
    "**Proceso puntual**: Es un proceso con $T$ continuo y $S$ discreto. Un caso particular son los Procesos de Poisson. \n",
    "\n",
    "**Sucesión de variables faleatorias continuas**: Es un proceso con $T$ es discreto y $S$ es continuo.\n",
    "\n",
    "**Proceso continuo**: Si T y S son continuos.\n",
    "\n",
    "**Propiedades de Markov**: Dado el proceso estocástico discreto $X_0, X_1, X_2, . . . , X_n, . . .$, donde $X_i$ indica el estado del sistema en el tiempo $i$, el proceso cumple la propiedad de Markov, $X$ es un *proceso de Markov* o *proceso Markoviano*, si $P (X_{n+1} \\leq x | X_n, X_{n-1}, . . . , X_0) = P (X_{n+1} \\leq x | X_n).$\n",
    " \n",
    "**Cadena de Markov**: Es un proceso de Markov con un conjunto discreto de estados $S$. En este caso la propiedad de Markov se puede escribir también como: $P (X_{n+1} = j | X_n = x_n, X_{n-1} = x_{n-1}, . . . , X_0 = x_0) = P (X_{n+1} = j | X_n = x_n), \\quad \\forall j, x_0, x_1,. . . , x_n \\in S$.\n",
    "\n",
    "## Probabilidades de transición\n",
    "Dada una cadena de Markov $X_n$_{n\\ge0}$ con un conjunto de estados $S=\\{1,2,. . . \\}$, se llama así a los números $P(X_{n+1}=j|X_n=i)$, donde $i,j\\in S$.\n",
    "\n",
    "**Cadena de Markov homogénea**: Es una cadena de Markov donde las probabilidades de transición no dependen del tiempo $n$. Usaremos la notación $p_{ij}=P(X_{n+1}=j|X_n=i)$, y como solo consideraremos este tipo de cadenas, veremos que verifican las siguientes propiedades:\n",
    "* $p_{ij}\\ge0$, para todo $i,j\\in S$.\n",
    "* $\\sum_{j\\in S}p_{ij}=1$, para todo $i\\in S$.\n",
    "\n",
    "**Distribución inicial**: Es la función $\\pi(j)=P(X_0=j)$, para todo $j\\in S$, tal que verifica las siguientes propiedades:\n",
    "* $\\pi(j)\\ge0$, para todo $j\\in S$.\n",
    "* $\\sum_{j\\in S}\\pi(j)=1$.\n",
    "\n",
    "Así, dada la distribución inicial y las probabilidades de transición, la cadena de Markov queda unívocamente determinada. En particular, la distribución conjunta de $X_0, X_1,. . . , X_n$ viene dada por $P(X_0 = x_0, X_1 = x_1, . . . , X_n = x_n) = \\pi(x_0)p_{x_0x_1}p_{x_1x_2}. . . p_{x_{n-1}x_n}$.\n",
    "\n",
    "**Matriz de transición**: La matriz de transición de una cadena de Markow homogénea con un número finito de estados $S=\\{1,2,. . . , N\\}$ es la matriz $(N+1)\\times(N+1)$ definida por:\n",
    "\n",
    "$$Q=(p_{ij})=\n",
    "\\begin{pmatrix}\n",
    "p_{00} & p_{01} & p_{02} & . . . \\\\\n",
    "p_{10} & p_{11} & p_{12} & . . . \\\\\n",
    "p_{20} & p_{21} & p_{22} & . . . \\\\\n",
    "... & ... & ... & . . . \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "donde $0\\le p_{ij}\\le1$ y los ejemento de una fila suman 1. Seguiremos utilizando la notación $p_{ij}^{(n)}$ para indicar la entrada matricial $(i,j)$ de la matriz $Q^n$.\n",
    "\n",
    "**Teorema - Ecuaciones de Chapman Kolmogorov**: Sea $Q = (p_{ij}) la matriz de transición de una cadena de Markov. Entonces la matriz {Q^n} contiene las probabilidades de transición en $n$ etapas. Esto es $(Q^n)_{ij} = P(X_{t+n} = j | X_t = i) = P(X_n = j | X_0 = i)$.\n",
    "\n",
    "## Diagrama de transición\n",
    "Dada una cadena de Markov con un conjunto finito de estados, el diagrama de transición de la cadena es un grafo dirigido cuyos nodos(vértices) son los estados de la cadena y cuyos arcos(aristas) se etiquetan con la probabilidad de transición entre los estados que ellos unen. Si $p_{ij} = 0$ entonces no existe arco del nodo $i$ al nodo $j$.\n",
    "\n",
    "### Ejemplo\n",
    "Supongamos que una máquina puede tener dos estados: en condiciones de operar (estado 0) y descompuesta (estado 1). Las probabilidades de transición están dadas por:\n",
    "\n",
    "$$p_{01} = P(0,1) = P(X_{n+1}=1|X_n=0)=0.15$$\n",
    "$$p_{10} = P(1,0) = P(X_{n+1}=0|X_n=1)=0.75$$\n",
    "\n",
    "Luego la matriz de transición es:\n",
    "\n",
    "$$Q= \\begin{pmatrix} 0.85 & 0.15 \\\\ 0.75 & 0.25 \\end{pmatrix}$$\n",
    "\n",
    "Así, si el estado inicial es 0, la máquina tiene una probabilidad del 15% de romperse y un 85% de permanecer operativa. Si la máquina está rota tiene un 25% de permanecer rota y un 75% de ser reparada.\n",
    "Las matrices $Q$, $Q^2$, $Q^3$ y $Q^4$ están dadas por:\n",
    "\n",
    "$$Q= \\begin{pmatrix} 0.85 & 0.15 \\\\ 0.75 & 0.25 \\end{pmatrix} \\quad \n",
    "Q^2= \\begin{pmatrix} 0.835 & 0.165 \\\\ 0.825 & 0.175 \\end{pmatrix} \\quad \n",
    "Q^3= \\begin{pmatrix} 0.8335 & 0.1665 \\\\ 0.8325 & 0.1675 \\end{pmatrix} \\quad \n",
    "Q^4= \\begin{pmatrix} 0.83335 & 0.16665 \\\\ 0.83325 & 0.16675 \\end{pmatrix}$$\n",
    "\n",
    "Observemos que si la máquina está inicialmente descompuesta ($X_0=1$), entonces en el paso 4 tendremos:\n",
    "\n",
    "$$P(X_4=1,X_0=1)=P(X_4=1|X_0=1)P(X_0=1)=0.16674\\cdot1\\approx17\\%$$\n",
    "\n",
    "En esta cadena, la máquina tiende a tener una probabilidad del 83% de estar reparada y un 17% de romperse, independientemente del estado inicial. Para tener una expresión exacta de las probabilidades $p_{ij}^{(n)}$ puede utilizarse la propiedad de los autovalores de la matriz $Q$ cuando esta es diagonalizable. En este ejemplo la matriz $Q$ tiene dos autovalores: 1 y 0.1. Esto indica que para cada par $i,j$ existen números $a,b\\in\\mathbb{R}$ tales que:\n",
    "\n",
    "$$p_{ij}^{(n)}=a\\cdot1^n+b\\cdot(0.1)^n$$\n",
    "\n",
    "Dado que $Q^0$ es la matriz identidad y $Q^1=Q$, reemplazamos los valores conocidos para $n=0$ y $n=1$ para obtener $a$ y $b$. En este caso tendremos por ejemplo que:\n",
    "\n",
    "$$\\begin{cases} p_{00}^0=1=a+b \\\\ p_{00}^1=0.85=a+0.1b \\end{cases}$$\n",
    "\n",
    "cuya solución es $a=\\frac{5}{6}$, $b=\\frac{1}{6}$. De esta manera tenemos que $p_{00}^{(n)}=\\frac{5}{6}+\\frac{1}{6}\\cdot(0.1)^n$, para todo $n\\in\\mathbb{N}$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.ibb.co/D7bvBTT/Captura-desde-2024-06-11-18-17-16.png\" alt=\"Diagrama de transicion\">\n",
    "    <p><em>Diagrama de transicion</em></p>\n",
    "</div>\n",
    "\n",
    "## Estructura de clases\n",
    "\n",
    "**Estado accesible**: Dada una cadena de Markov $(X_n)_{n\\ge0}$, diremos que el estado $j$ es accesible desde $i$, o que puede ser alcanzado desde $i$, si hay una probabilidad positiva de que la cadena iniciada en $i$ llegue al estado $j$. Esto es: $P(X_n=j\\text{ para algún }n\\ge0|X_0=i)=p_{ij}^{(n)}>0$.\n",
    "\n",
    "**Comunicación entre estados**: Decimos que dos estados $i$ y $j$ se comunican entre sí $j$ es accesible desde $i$ e $i$ es accesible desde $j$. Esta relación divide al espacio de estados $S$ en distintas **clases comunicantes**.\n",
    "\n",
    "**Estado cerrado**: Un subconjunto $C$ no vacío de estados de una cadena de Markov se dice cerrado si y solo si para todo $i\\in C$ y $j\\notin C$, $j$ no es alcanzable desde $i$. Así, una cadena que alcanza un estado en $C$ siempre permanecerá en $C$.\n",
    "\n",
    "**Conjunto irreducible**: Un subconjunto $C\\subseteq S$ se dice irreducible si y solo si no contiene ningún subconjunto propio cerrado. Una cadena de Markov se dice irreducible si su espacio de estados $S$ es irreducible.\n",
    "\n",
    "## Clasificación de estados\n",
    "\n",
    "$v_{ij}=P(X_n=j\\text{ para algún }n\\ge1|X_0=i)$ denotará la probabilidad de que la cadena de Markov que comienza en $i$ llegue al estado $j$ en un tiempo positivo.\n",
    "\n",
    "**Camino**: Decimos que existe un camino de $i$ a $j$ si $v_{ij}>0$. Si $v_{ij}=1$ significa que partiendo del estado $i$ cualquier camino pasa por el estado $j$. Mientras que si $v_{ij}<1$ entonces hay al menos un camino que parte de $i$ y desde el cual no es posible llegar a $j$. A partir de esta notación, definimos los siguientes tipos de estados:\n",
    "* **Estado recurrente**: Estado $j$ tal que $v_{jj}=1$. Se clasifican a su vez en \n",
    "    * *Estados recurrentes positivos*: Aquellos cuyo tiempo medio hasta retornar al estado es finito.\n",
    "    * *Estados recurrentes nulos*: Caso contrario. En el caso de cadenas con un número finito de estados sólo pueden darse estados recurrentes positivos.\n",
    "* **Estado transitorio**: Estado no recurrente.\n",
    "* **Estado absorbente**: Estado recurrente $j$ tal que $p_{jj}=1$.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://i.ibb.co/2PWTrKX/Captura-desde-2024-06-11-23-12-19.png\" alt=\"Estado absorbente\">\n",
    "    <p><em>Estado absorbente</em></p>\n",
    "</div>\n",
    "\n",
    "**Cadena recurrente**: Una cadena de Markov se dice recurrente si todos sus estados son recurrentes, y se dice *transitoria* si todos sus estados son transitorios. Si una cadena de Markov tiene un número finito de estados entonces no puede ser transitoria. Debe tener algún estado recurrente.\n",
    "\n",
    "**Proposición**: Si $i$ es un estado recurrente y existe un camino de $i$ a $j$, entonces $j$ es recurrente y $v_{ij}=v_{ji}=1$.\n",
    "\n",
    "## Cadenas periódicas\n",
    "\n",
    "Si $i$ es un estado recurrente de una cadena significa que existe un $n>1$ tal que $P(X_n=i|X_0=i)>0$. Si esta probabilidad es positiva para valores positivos $n$ y $m$, también lo es para cualquier combinación $h\\cdot m+l\\cdot n$, con $h$ y $l$ enteros no negativos. En particular, el *período* del estado $i$ se define como:\n",
    "\n",
    "$$k=\\text{MCD}\\{n>0:P(X_n=i|X_0=i)>0\\}$$\n",
    "\n",
    "donde MCD indica el máximo común divisor.\n",
    "\n",
    "**Estado aperiódico**: Se dice a un estado tal que $k=1$. Se puede probar que en una cadena irreducible, o todos los estados tienen un mismo período $k$ o bien todos son aperiódicos.\n",
    "\n",
    "**Estado periódico**: Se dice *periódico* de período $k$ a un estado tal que $k>1$. Observamos que si la cadena es finita, entonces si es irreducible también es recurrente. Sin embargo si la cadena es infinita podría ser irreducible, aperiódica y transitoria.\n",
    "\n",
    "## Tiempo de alcance y probabilidad de absorción\n",
    "\n",
    "Consideremos una cadena de Markov $(X_n)_{n\\ge0}$ con espacio de estados $S$ y probabilidades de transición $p_{ij}$. El tiempo mínimo de alcance de un subconjunto $A$ de $S$ es la variable aleatoria $H_A$ dada por:\n",
    "\n",
    "$$H^A=\\inf\\{n\\ge0|X_n\\in A\\}$$\n",
    "\n",
    "donde acordamos que el ínfimo del conjunto vacío $\\emptyset$ es $\\infty$. La probabilidad de alcanzar $A$ desde el estado $i$ se define como:\n",
    "\n",
    "$$h^A_i=P(H^A<\\infty|X_0=i)$$\n",
    "\n",
    "Si $A$ es un conjunto cerrado, entonces $h^A_i$ se dice una *probabilidad de absorción*. Si $A=\\{j\\}$, entonces $h^{\\{j\\}}_i$ es la suma de las probabilidades de que en el $n$-ésimo paso se alcance por primera vez el valor $j$:\n",
    "\n",
    "$$h^{\\{j\\}}_i=\\sum_{n\\ge0}P(H^A=n|X_0=i)$$\n",
    "\n",
    "El tiempo medio de alcance del conjunto $A$ desde el estado $i$ se define como la esperanza condicional:\n",
    "\n",
    "$$k^A_i=E[H^A|X_0=i]=\\begin{cases} \\sum^\\infty_{n=0}n\\cdot P(H^A=n|X_0=i), & \\text{si } P(H^A=\\infty|X_0=i)=0 \\\\ \\infty, & \\text{si } P(H^A=\\infty|X_0=i)>0 \\end{cases}$$\n",
    "\n",
    "Notemos que $h_i$ y $k_i$ están definidas por una suma infinita, en la cual se consideran todas las cadenas iniciadas en $i$ y que eventualmente llegan al conjunto $A$. Esto hace que pueda resultar complejo su cálculo. Una forma alternativa de calcular la probabilidad de alcance y el tiempo medio es a través de un sistema de ecuaciones. Enunciaremos los teoremas que quedarán sin demostración: En primer lugar, si $S$ es el conjunto de estados entonces llamaremos vector de probabilidades de alcance y vector de tiempos medios de alcance a los conjuntos\n",
    "\n",
    "$$h^A=\\{h^A_i|i\\in S\\} \\quad y \\quad k^A=\\{k^A_i|i\\in S\\}.$$\n",
    "\n",
    "**Teorema**: El vector de probabilidades de alcance $h^A$ es la solución mínima no negativa del sistema de ecuaciones lineales:\n",
    "\n",
    "$$\\begin{cases} h^A_i=1, & i\\in A \\\\ h^A_i=\\sum_{j\\in S}p_{ij}h^A_j, & i\\notin A \\end{cases}$$\n",
    "\n",
    "**Teorema**: El vector de tiempos medios de alcance $k^A$ es la solución mínima no negativa del sistema de ecuaciones lineales:\n",
    "\n",
    "$$\\begin{cases} k^A_i=0, & i\\in A \\\\ k^A_i=1+\\sum_{j\\in S}p_{ij}k^A_j, & i\\notin A \\end{cases}$$\n",
    "\n",
    "## Tiempo medio de retorno\n",
    "\n",
    "Dada una cadena de Markov $(X_n)$ y $j$ un estado recurrente de la cadena, el tiempo medio de retorno se define como el valor esperado del tiempo de alcance del estado $j$ sobre todos los caminos de al menos un paso. Es decir, dado el conjunto:\n",
    "\n",
    "$$R^{(j)}=\\inf\\{n\\ge1|X_n=j\\}$$\n",
    "\n",
    "se pretende determinar $r_j=E[R^{(j)}|X_0=j]$. Este valor esperado está definido entonces por:\n",
    "\n",
    "$$r_j=\\sum^\\infty_{n=1}nP(R^{(j)}=n)$$\n",
    "\n",
    "También puede determinarse a partir de las probabilidades de alcance desde cada uno de los estados, viendo que cada camino de $j$ a $j$ de longitud mayor o igual a 1, tiene una longitud 1 más la longitud de un camino que comienza en el estado adoptado por $X_1$. Así, denotando con $k^{(j)}_i$ al tiempo esperado de alcance desde el estado $i$ al estado $j$, tenemos que el tiempo medio de retorno al estado $j$ es:\n",
    "\n",
    "$$r_j=1+\\sum_{i\\in S}p_{ji}k^{(j)}_i$$\n",
    "\n",
    "## Distribución estacionaria\n",
    "\n",
    "Dada una cadena de Markov con probabilidades de transición $Q$, decimos que $\\pi$ es una distribución estacionaria si se cumple que $\\pi Q=\\pi$. No todas las cadenas de Markov tienen distribución estacionaria. Por ejemplo, una cadena infinita con estados $\\{0,1,2,...\\}$ donde $p_{i,i+1}=1$ para todo $i$ no lo tiene. Otras cadenas de Markov tienen más de una distribución estacionaria, por ejemplo si $Q$ es la matriz identidad. En el caso de cadenas finitas siempre existe al menos una. En una cadena de Markov $X_0, X_1, X_2,...$, cada $X_t$ es una variable aleatoria con una cierta distribución. Para ciertas cadenas de Markov ocurre que para valores grandes de $t$ estas distribuciones convergen a una distribución dada.\n",
    "\n",
    "Por otra parte hemos visto que en ciertos casos ocurre que \n",
    "\n",
    "$$\\lim_{ij}q_{ij}^{(n)}=p_j,$$\n",
    "\n",
    "es decir que las probabilidades de transición en $n$ pasos del estado $i$ al estado $j$ no dependen de $i$. En este caso y si los valores $p_j$ determinan una distribución de probabilidad, se puede probar que estas probabilidades límite constituyen también una distribución estacionaria.\n",
    "\n",
    "Definimos una cadena de Markov como ergódica si es irreducible, aperiódica y recurrente. En el caso de cadenas de Markov finitas, si es irreducible también es recurrente. Por lo cual una cadena ergódica es una cadena irreducible y aperiódica.\n",
    "\n",
    "**Teorema**: Si $X$ es una cadena de Markov ergódica, entonces para todo estado $j\\in S$ existe el límite\n",
    "\n",
    "$$\\lim_{n\\to\\infty}p_{ij}^{(n)}=p_j$$\n",
    "\n",
    "En términos de la matriz de transición, significa que las filas de la matriz $Q^n$ tienden todas a un mismo vector.\n",
    "\n",
    "Se dice que una cadena de Markov alcanza una distribución estacionaria si existe el límite del teorema anterior y además $X_{j\\in S}p_j=1$. Si bien este límite existe, puede ocurrir que el límite sea 0 para todo $j$ por lo cual no converge a un vector de probabilidad o distribución estacionaria. Esto puede ocurrir en particular si la cadena es infinita. El siguiente teorema da condiciones suficientes para que una cadena de Markov alcance una distribución estacionaria. Si $S=\\{0,1,2,...,N\\}$ y $\\pi$ es la distribución estacionaria, denotamos $q_j=\\pi(j)$.\n",
    "\n",
    "**Teorema**: Si $X$ es una cadena de Markov finita y ergódica, entonces la distribución estacionaria existe y viene dada por las siguientes ecuaciones:\n",
    "\n",
    "$$q_j=\\sum^N_{i=0}q_i p_{ij},\\quad j=0,1,...,N$$\n",
    "\n",
    "$$\\sum^N_{i=0}q_i=1$$\n",
    "\n",
    "El teorema no solo da condiciones suficientes para la existencia de la distribución estacionaria sino que además da un método para hallarla. La ecuación dice que el vector $q$ de probabilidades satisface:\n",
    "\n",
    "$$q\\cdot Q=q$$\n",
    "\n",
    "y la ecuación indica que la suma de los elementos del vector es 1. En particular es suficiente con resolver la primera ecuación y luego normalizar el vector para que la suma de sus componentes sea 1.\n",
    "\n",
    "Observemos que si se cumple la primer ecuación del teorema entonces también es cierto que\n",
    "\n",
    "$$\\sum^N_{i=0}q_i p_{ij}=\\sum^N_{i=0}q_j p_{ji},\\quad 0\\le j\\le N$$\n",
    "\n",
    "puesto que la segunda sumatoria es igual a $q_j$. Las ecuaciones de arriba se llaman también ecuaciones de equilibrio. Esto no implica necesariamente que $q_j p_{ji}=q_i p_{ij}$, pero si esto ocurre se dice que la cadena de Markov es reversible."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
