{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capítulo 7: Análisis de Datos Simulados\n",
    "Para simular un sistema real es necesario:\n",
    "* Representar cada fuente de aleatoriedad de acuerdo a una distribución de probabilidad.\n",
    "* Elegir adecuadamente la distribución, para no afectar los resultados de la simulación.\n",
    "\n",
    "## Selección de una distribución\n",
    "\n",
    "Para elegir una distribución es necesario trabajar con datos obtenidos del sistema real a simular. Estos datos pueden luego ser usados:\n",
    "\n",
    "**Directamente**: Pero solo se podrán reproducir datos históricos y resulta una información insuficiente para realizar buenas simulaciones del modelo.\n",
    "\n",
    "**Realizando el muestreo a partir de la distribución empírica de los datos**: Permite reproducir datos intermedios a los datos observados, lo cual es algo deseable fundamentalmente si se tienen datos de tipo contínuo. Esta técnica es recomendable en los casos en que no se puedan ajustar los datos a una distribución teórica.\n",
    "\n",
    "\n",
    "* Caso discreto: Si los datos observados son $X_1,...,X_N$, la distribución empírica asigna una función de masa de probabilidad empírica a cada $x$ dada por\n",
    "$$p_e(x)=\\frac{\\#\\{i|X_i=x,1\\le i\\le n\\}}{n} \\quad \\Rightarrow \\quad F_e(x)=\\frac{\\#\\{i|X_i\\le x,1\\le i\\le n\\}}{n}.$$\n",
    "\n",
    "* Caso discreto: Si los datos observados son $X_1,...,X_N$, una posibilidad es definir $F(x) = Fe(x)$ en los puntos observados, y unir con una poligonal en los puntos intermedios a las observaciones. Esto es, en primer lugar se ordenan los valores en forma creciente, denotando $X_{(i)}$ a la observación que ocupa el i-ésimo lugar en el ordenamiento, $X_{(1)}<X_{(2)}<...<X_{(n)}$. Y, la distribución propuesta es:\n",
    "\n",
    "$$F_{el}(x)=\\begin{cases}\n",
    "0 & \\text{si } x<X_{(1)}\\\\\n",
    "\\frac{i-1}{n-1}+\\frac{x-X-{(i)}}{(n-1)(X_{(i+1)}-X_{(i)})} & \\text{si } X_{(i)}\\le x<X_{(i+1)}\\\\\n",
    "1 & \\text{si } x\\ge X_{(n)}\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Por otro lado, si lo que se conoce es una agrupación de los datos en distintos intervalos $[a_1, a_2), [a_2, a_3), . . . , [a_{k−1}, a_k)$, es decir, un *histograma de los datos*, se puede hacer una distribución empírica que aproxime a la frecuencia acumulada de las observaciones. Esto es, si $n_j$ es la cantidad de observaciones en el intervalo $[a_j , a_{j+1})$, entonces $n = n_1 + n_2 + · · · + n_k$, y se define la distribución empírica lineal G, donde\n",
    "\n",
    "$$G(a_1)=0, \\quad G(a_j)=\\frac{n_1 + n_2 + · · · + n_{j−1}}{n}, \\quad 2\\le j\\le k+1,\n",
    "\\quad\\Rightarrow\\quad\n",
    "G(x)=\\begin{cases}\n",
    "0 & \\text{si } x<a_1\\\\\n",
    "G(a_j)+\\frac{G(a_{j+1})-G(a_j)}{a_{j+1}-a_j}(x-a_j)& \\text{si } a_j<x<a_{j+1}, 1\\le j \\le k\\\\\n",
    "1 & \\text{si } x\\ge a_{k+1}\n",
    "\\end{cases}.$$\n",
    "\n",
    "**Utilizando técnicas de inferencia estadística**: Tienen una forma más suave. Además pueden simularse datos aún fuera del rango de los datos observados. No es necesario almacenar los datos observados ni las correspondientes probabilidades acumuladas. Se pueden modificar fácilmente los parámetros de la distribución elegida. Puede ser que no se encuentre una distribución adecuada para los datos observados, generandose valores extremos no deseados.\n",
    "\n",
    "## Algunas medidas estadísticas\n",
    "A la hora de seleccionar una determinada distribución teórica de probabilidad para llevar  adelante una simulación, es importante conocer algunos valores estadísticos que tienen las distribuciones teóricas y compararlos con los que se obtienen a partir de una muestra. Ahora bien, estos valores estan bien definidos para una distribución teórica pero son desconocidos para una distribución de la cual sólo se conoce una muestra. Entonces, para estimar estos valores, se utilizan los estadísticos muestrales. Un **estadístico muestral** es una variable aleatoria definida a partir de los valores de una muestra.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "    \\hline\n",
    "    \\text{Función} & \\text{Estimador muestral} & \\text{Estima} \\\\ \\hline\n",
    "    \\text{Min,Max} & X_{(1)},X_{(n)} & \\text{Rango} \\\\ \\hline\n",
    "    \\text{Media } \\mu & \\bar{X}(n) & \\text{Tendencia central} \\\\ \\hline\n",
    "    \\text{Mediana} & \n",
    "        \\hat m=\\begin{cases} X_{(n+1)/2}\\\\\n",
    "        1/2(X_{(n/2)}+X_{(n/2+1)}) \\end{cases} & \\text{Tendencia central} \\\\ \\hline\n",
    "    \\text{Varianza } \\sigma^2 & S^2(n) & \\text{Variabillidad} \\\\ \\hline\n",
    "    c.v.=\\frac\\sigma\\mu & \\hat {cv}=\\frac{\\sqrt{S^2 (n)}}{{\\bar{X}(n)}} & \\text{Variabillidad} \\\\ \\hline\n",
    "    \\tau & \\hat \\Tau (n)=frac{{S^2 (n)}}{{\\bar{X}(n)}} & \\text{Variabillidad} \\\\ \\hline\n",
    "    \\text{Asimetría } \\nu=\\frac{E[(X-\\mu)^3]}{(\\sigma^2)^{3/2}} & \\hat\\nu(n)=\\frac{\\Sigma_i(X_i\\\\bar X (n)))^3/n}{[S^2(n)]^{3/2}} & \\text{Simetría} \\\\ \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "## Estimación de parámetros\n",
    "\n",
    "**Estimador**: Dada una muestra de $n$ datos observados, se llama *estimador* $\\hat\\theta$ del parámetro $\\theta$ a cualquier función de los datos observados. Un buen estimador debe cumplir con las siguientes propiedades:\n",
    "* **Insesgabilidad**: Se dice que el estimador es insesgado si $E[\\hat\\theta]$\n",
    "* **Consistencia**: Si al aumentar la muestra, el estimador se aproxima al parámetro.\n",
    "* **Eficiencia**: Se calcula comparando su varianza con la de otro estimador. Cuanto menor es\n",
    "la varianza, se dice que el estimador es mas eficiente.\n",
    "* **Suficiencia**: Significa que el estimador utiliza toda la informacion obtenida de la muestra.\n",
    "\n",
    "**Error cuadrático medio de un estimador (ECM)**: Es una medida de diseprción entre el estimador y el parámetro a estimar donde si el estimador es insesgado, el ECM es igual a la varianza del estimador. Dado $\\hat\\theta$ un estimador de $\\theta$ de una distribución $F$, $ECM(\\hat\\theta,\\theta)=E[(\\hat\\theta-\\theta)^2].$\n",
    "\n",
    "**Estimadores de máxima verosimilitud (MLE)**: El estimador de máxima verosimilitud de un parámetro (o un conjunto de parámetros) $θ$, asume que la muestra obtenida tiene máxima probabilidad de ocurrir entre todas las muestras posibles de tamaño $n$, y que los datos $X_1, X_2, . . . , X_n$ son independientes. Supongamos que se tiene la hipótesis de una distribución para los datos observados, y se desconoce un parámetro $θ$. Entonces, dado que se han observado datos  $X_1, X_2, . . . , X_n$, se define la función de máxima verosimilitud $L(θ)$ de manera que el estimador de máxima verosimilitud es el valor de $\\hatθ$ que la maximiza, es decir, $L(\\hat\\theta)\\ge L(\\theta), \\quad \\theta\\text{ valor posible}.$\n",
    "\n",
    "* Caso discreto: Sea $p_θ(x)$ la probabilidad de masa para dicha distribución, $L(θ) = p_θ(X_1)p_θ(X_2) . . . p_θ(X_n).$\n",
    "* Caso contínuo: Sea $f_θ(x)$ la función de densidad para dicha distribución, $L(θ) = f_θ(X_1)f_θ(X_2) . . . f_θ(X_n).$\n",
    "\n",
    "## Estimación con Simulaciones\n",
    "La media muestral y la varianza muestral son estimadores que dependen del tamaño de la muestra. Además de un valor esperado, puede querer estimarse la probabilidad de que un evento suceda, la cual también pueden ser estimadas con el estimador de media muestral, pero aplicado a otra variable particular. Nuevamente, incrementar el número de simulaciones mejorará la estimación de esta probabilidad. Veamos con qué criterio podemos elegir un número aceptable de simulaciones.\n",
    "\n",
    "### Simulación de media muestral\n",
    "Supongamos que con un programa de simulación es posible generar sucesivamente datos ${X_i}$, independientes, y se desea estimar la media $\\mu$ de los datos, es decir $\\mu = E[X_i]$. Entonces para un determinado $n$ se podrá tomar el valor $\\bar X(n)$ como una estimación de la media $\\mu$. Sabemos por el Teorema Central del Límite, que\n",
    "\n",
    "$$Z = \\frac{\\bar X(n)-\\mu}{\\sigma/\\sqrt{n}}\\dot\\sim N(0,1)\n",
    "\\quad \\Rightarrow \\quad\n",
    "P\\Big(|\\bar X(n)-\\mu|<c\\frac{\\sigma}{\\sqrt{n}}\\Big)\\sim P(|Z|<c).$$\n",
    "\n",
    "Entonces, el valor $c$ tendrá que ver con el nivel de certeza de la aproximación a $\\mu$. Así, un procedimiento para determinar hasta que valor de $n$ deben generarse datos $X_n$ es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def Media_Muestral_X(d,simularX,actualizar):\n",
    "    'Estimación de X(n) con ECM d'\n",
    "    mediaX = simularX() #X(1)\n",
    "    Scuad, n = 0, 1 #Scuad = Sˆ2(1)\n",
    "    # Hasta que la muestra sea mayor a 100 (para aproximare a una normal)\n",
    "    # y el ECM o varianza del estimador sigma^2/n sea menor que d\n",
    "    while n<=100 or sqrt(Scuad/n)>d:\n",
    "        n += 1\n",
    "        X = simularX()\n",
    "        mediaX, Scuad = actualizar(X, mediaX, Scuad) #X(n), Sˆ2(n)\n",
    "    return mediaX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fórmulas recursivas\n",
    "En el algoritmo anterior, se debe calcular la media muestral y la varianza muestral en cada iteración. Por ello, es conveniente tener un método iterativo que permita calcular $\\bar X(n)$ y $S(n)$ en cada paso, sin reutilizar todos los valores generados previamente. Para ello, se pueden utilizar las siguientes fórmulas recursivas:\n",
    "\n",
    "$$ \\bar X(n+1) = \\bar X(n) + \\frac{X_{n+1} - \\bar X(n)}{n+1}, \\quad S^2(n+1) = \\bigg(1-\\frac1{n}\\bigg) S^2(n) + (n+1)(\\bar X(n+1)-\\bar X(n))^2.$$\n",
    "\n",
    "\n",
    "Así, repasando las ideas, el estimador media muestral se calcula iteradamente para estimar el valor esperado de los valores simulados. Esta iteración prosigue hasta que el error cuadrático medio o varianza del estimador, $\\sigma^2/n$, es menor a un valor $d$ deseado. En caso de no conocerse $\\sigma$ se estima su valor con el estimador $S^2(n)$. Así el algoritmo resulta\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Media_Muestral_X(d, simularX):\n",
    "    'Estimación del valor esperado con ECM<d'\n",
    "    Media = simularX() # X(1)\n",
    "    Scuad, n = 0, 1 #Scuad = Sˆ2(1)\n",
    "    # Hasta que la muestra sea mayor a 100 (para aproximare a una normal)\n",
    "    # y el ECM o varianza del estimador sigma^2/n sea menor que d\n",
    "    while n <= 100 or sqrt(Scuad/n) > d:\n",
    "        n += 1\n",
    "        X = simularX()\n",
    "        MediaAnt = Media\n",
    "        Media = MediaAnt + (X - MediaAnt) / n\n",
    "        Scuad = Scuad * (1 - 1 /(n-1)) + n*(Media - MediaAnt)**2\n",
    "    return Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimador de proporción\n",
    "El estimador $\\bar X(n)$ puede utilizarse también para estimar la proporción de casos en una población, o la probabilidad de ocurrencia de un evento. En este caso, en la $i$-ésima simulación se tendrá una variable de Bernoulli $X_i$ con el valor 1 si ocurre el evento y 0 en caso contrario. Así $E[X_i] = P(X_i = 1) = P(\\text{ocurrencia del evento}).$ Por lo tanto la media muestral es en este caso un estimador de esta proporción. El objetivo será entonces estimar la probabilidad $p$ de éxito, y el estimador de $p$ será la media muestral: $\\hat p(n) = \\bar X(n),$ donde $n$ será el número total de simulaciones y dependerá de la precisión con que se quiera estimar $p$, en particular, del error cuadrático medio aceptable para el estimador\n",
    "$σ^22/n$. Ahora bien, en el caso de una variable Bernoulli, la varianza $σ^2$ es p(1 − p), y por lo tanto un estimador para la varianza de la variable simulada es $\\hatσ^2= \\bar X(n)(1 − \\bar X(n)),$ y un estimador del error cuadrático medio del estimador, que coincide con su varianza es:\n",
    "\n",
    "$$ECM(\\hat p(n), p) = Var(\\hat p(n)) = \\frac{\\bar X(n)(1 − \\bar X(n))}{n}.$$\n",
    "\n",
    "Así, si $X_1,...,X_n$, es una sucesión de v.a. independientes, Bernoulli, el algoritmo para la estimación de $p=E(X_i)$ es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimador_p(d, SimularX):\n",
    "    'Estimación de proporción con ECM<d'\n",
    "    p = 0\n",
    "    n = 0\n",
    "    while n <= 100 or sqrt( p * (1-p) / n) > d:\n",
    "        n += 1\n",
    "        X = SimularX()\n",
    "        p = p + (X - p) / n\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimador por intervalos\n",
    "**Estimador por intervalo**: Es un intervalo para el que se predice que el parámetro está contenido en él. Es decir, en este caso se tiene un intervalo aleatorio con una cierta probabilidad de contener al parámetro buscado. La confianza que se da al intervalo es la probabilidad de que el intervalo contenga al parámetro. \n",
    "\n",
    "### Estimador por intervalo de $E(X)$\n",
    "El estimador $X(n)$ es un estimador puntual del valor esperado de $X$. Además sabemos que si $E(X) = θ$ y $Var(X) = σ^2$, entonces la distribución de $X(n)$ tiende a una normal estándar:\n",
    "\n",
    "$$\\frac{\\bar X(n) − θ}{σ/\\sqrt{n}} \\sim N(0, 1).$$\n",
    "\n",
    "Recordemos que para $0<α<1$, utilizamos la notación $z_α$ para indicar el número real tal que $P(Z < z_α) = α$. Luego, dado que la normal estándar tiene una distribución simétrica respecto a $x=0$, para $n$ suficientemente grande (>100), tenemos que \n",
    "\n",
    "$$P(−z_{α/2} < Z < z_{α/2}) =\n",
    "P(\\bar X(n) − z_{α/2} \\frac{σ}{\\sqrt{n}} < θ < \\bar X(n) + z_{α/2}\\frac{σ}{\\sqrt{n}}) = 1 − α.$$\n",
    "\n",
    "Esta ecuación determina un intervalo que contiene al parámetro $\\theta$ con una **confianza** de $1-\\alpha$. Si $σ$ es desconocido, los intervalos anteriores se definen utilizando el estimador $\\hat σ = \\sqrt{S^2(n)}$.\n",
    "\n",
    "Notemos que si la muestra es de tamaño $n$, la longitud del intervalo de confianza del $100(1 − α) %$ es\n",
    "\n",
    "$$l = 2 · z_{α/2} \\frac{σ}{\\sqrt{n}} \\quad \\text{o} \\quad l = 2 · z_{α/2} \\frac{\\hat σ}{\\sqrt{n}},$$\n",
    "\n",
    "es decir que su longitud depende del valor de $n$, y mas especÍficamente, es inversamente proporcional al valor de $n$. Así, en una simulación, si se quiere obtener un intervalo de confianza del $100(1 − α) %$ y con una longitud menor a cierto número $L$, se continuarán generando valores hasta que\n",
    "\n",
    "$$2 · z_{α/2} \\frac{S(n)}{\\sqrt n} < L.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Media_Muestral_X(z_alfa_2, L, simularX): #z_alfa_2 = z_(alfa/2)\n",
    "    'Confianza = (1 - alfa)%, amplitud del intervalo: L'\n",
    "    d = L / (2 * z_alfa_2)\n",
    "    Media = simularX()\n",
    "    Scuad, n = 0, 1\n",
    "    while n <= 100 or sqrt(Scuad / n) > d:\n",
    "        n += 1\n",
    "        X = simularX()\n",
    "        MediaAnt = Media\n",
    "        Media = MediaAnt + (X - MediaAnt) / n\n",
    "        Scuad = Scuad * (1 - 1 /(n-1)) + n*(Media - MediaAnt)**2\n",
    "    return Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Estimador por intervalos de una proporción\n",
    "En el caso de una variable Bernoulli, el estimador por intervalos del parámetro $p$ también es el estimador por intervalos de la media poblacional. En este caso, el estimador para la varianza es $X(n)(1 − \\bar X(n))$, y para $n$ suficientemente grande se tiene que\n",
    "\n",
    "$$ \\frac{\\bar X(n)}{\\sqrt{\\frac{\\bar X(n)(1 − \\bar X(n))}{n}}} = Z \\sim N(0, 1).$$\n",
    "\n",
    "Así, un intervalo de confianza del $100(1 − α) %$ se obtiene a partir de la propiedad:\n",
    "\n",
    "$$P(−z_{α/2} < Z < z_{α/2})=$$\n",
    "$$P\\Big(\\bar X(n) − z_{α/2} \\sqrt{\\frac{\\bar X(n)(1 − \\bar X(n))}{n}} < p < \\bar X(n) + z_{α/2} \\sqrt{\\frac{\\bar X(n)(1 − \\bar X(n))}{n}}\\Big)=$$\n",
    "$$1 − α.$$\n",
    "\n",
    "o equivalentemente, el intervalo de confianza es:\n",
    "\n",
    "$$\\Big(\\bar X(n) − z_{α/2} \\sqrt{\\frac{\\bar X(n)(1 − \\bar X(n))}{n}}, \\bar X(n) + z_{α/2} \\sqrt{\\frac{\\bar X(n)(1 − \\bar X(n))}{n}}\\Big).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimador_p(z_alfa_2,L,SimularX):\n",
    "    'Confianza: 100(1-alpha)%'\n",
    "    'L: amplitud del intervalo'\n",
    "    d = L / (2 * z_alfa_2)\n",
    "    p = 0; n = 0\n",
    "    while n <= 100 or sqrt(p * (1 - p) / n) > d:\n",
    "        n += 1\n",
    "        X = SimularX()\n",
    "        p = p + (X - p) / n\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La técnica de bootstrap\n",
    "**Muestra bootstrap**: Es una muestra aleatoria de tamaño $n$ tomada de $X$ a partir de la distribución $F_e$. Dicho de otro modo, es una muestra aleatoria de tamaño $n$ tomada con\n",
    "reposición del conjunto ${x_1, x_2, . . . , x_n}$.\n",
    "\n",
    "**Técnica bootstrap**: En términos generales, es aquella que recupera una información a partir de los datos, sin asumir ninguna hipótesis sobre ellos.\n",
    "\n",
    "**Estimación bootstrap**: Dado que el error cuadrático medio y la varianza del estimador se definen ambos como un valor esperado, la estimación bootstrap estima estos valores a partir de la distribución empírica de los datos y utilizando muestras bootstrap, $ECM(\\hatθ, θ) ∼ E_{F_e} [(\\hatθ − θ_{F_e})^2], \\quad Var(\\hatθ) = E_{F_e} [(\\hatθ − E_{F_e} [\\hatθ])^2].$\n",
    "\n",
    "**Estimaciones bootstrap ideales**: Del mismo modo se podría querer estimar una probabilidad $P(a < \\hatθ < b)$. Dado que esta probabilidad puede verse como el valor esperado de una variable aleatoria Bernoulli,\n",
    "\n",
    "$$X = \\begin{cases} 1 & \\text{si } a < \\hatθ < b\\\\ 0 & \\text{en otro caso} \\end{cases},$$\n",
    "\n",
    "entonces tambien puede realizarse una estimación bootstrap de la probabilidad:\n",
    "\n",
    "$$P(a < \\hatθ < b) \\sim \\frac1{n^n}\\#\\{(x_1^*, x_2^*, . . . , x_n^*)|a < \\hatθ(x_1^*, x_2^*, . . . , x_n^*) < b\\},$$\n",
    "\n",
    "donde el supraíndice ∗ indica que es una muestra bootstrap de ${x_1, . . . , x_n}$. Estas estimaciones se denominan estimaciones bootstrap ideales, ya que toman todas las muestras bootstrap posibles. Sin embargo, en la práctica esto es computacionalmente muy costoso ya que requiere realizar $n^n$ replicaciones bootstrap.\n",
    "\n",
    "### Estimación de bootstrap del ECM\n",
    "En el caso que se conozca la distribución $F$ de los datos, se podrá calcular con mayor o menor complejidad el valor exacto del ECM: $ECM(\\hatθ, θ) = E_F ((\\hatθ − θ)^2).$ Aquí el subíndice $F$ indica que el valor esperado se calcula en términos de esa distribución. Así, si la distribución $F$ de los datos es desconocida no es posible determinar de manera exacta este valor, y una alternativa es aproximarla con muestras bootstrap. Así, si la muestra es de tamaño $n$, y los datos obtenidos son $x_1, x_2, . . . , x_n$, el error cuadrático medio se calculará de la siguiente manera:\n",
    "\n",
    "1. Se calcula el parámetro $\\hatθ(F_e)$.\n",
    "\n",
    "2. Se generan $N$ muestras bootstrap y se calculan las respectivas replicaciones bootstrap del estimador. \n",
    "\n",
    "3. Por último, el error cuadrático medio para la distribución empírica es un valor esperado de las diferencias al cuadrado entre las realizaciones bootstrap y el parámetro a estimar.\n",
    "\n",
    "### Estimación de bootstrap de $Var(\\hat\\theta)$\n",
    "\n",
    "En el caso de la estimación boostrap de la varianza de un estimador $\\hatθ = \\hatθ(X1, X2, . . . , Xn)$, utilizamos el estimador varianza muestral. Es decir, si $\\hatθ$ es un estimador queremos calcular el valor: $E((\\hat\\theta-E(\\hat\\theta))^2),$ y que la técnica bootstrap aproxima con la distribución empírica. Esto es:$E_{F_e}((\\hat\\theta-E_{F_e}(\\hat\\theta))^2)\\sim E((\\hat\\theta-E(\\hat\\theta))^2)$. En la distribucion empírica, $\\hat\\theta$ toma valores en $n^n$ muestras posibles, por lo cual la varianza empírica del estimador puede no ser facilmente calculable y en ese caso se estima a su vez con la varianza muestral a partir de una muestra de tamaño $N$ de valores del estimador:\n",
    "\n",
    "$$\\frac1{N-1}\\sum_{i=1}^N(\\hat\\theta_i-\\bar\\theta)^2, \\quad \\bar{\\hat\\theta}=\\frac1N\\sum_{i=1}^N\\theta_i.$$\n",
    "\n",
    "Supongamos entonces que se tiene una muestra de tamaño $n$, $x_{i_1}, x_{i_2}, . . . , x_{i_n}$,para la estimación bootstrap se considerán $N$ muestras bootstrap $ b_1 = (x_{i_1}^{(1)}, x_{i_2}^{(1)}, . . . , x_{i_n}^{(1)}), b_2 = (x_{i_1}^{(2)}, x_{i_2}^{(2)}, . . . , x_{i_n}^{(2)}), . . . , b_N = (x_{i_1}^{(N)}, x_{i_2}^{(N)}, . . . , x_{i_n}^{(N)}),$ y se calculan las $N$ replicaciones bootstrap correspondientes $\\hat\\theta(b_1), \\hat\\theta(b_2), . . . , \\hat\\theta(b_N).$ Hasta aquí es el equivalente de haber tomado una muestra de tamaño $N$ de la variable aleatoria $\\hatθ$. Ahora se evalua la media muestral en la muestra obtenida:\n",
    "\n",
    "$$\\hat\\theta_m = \\frac1N\\sum_{j=1}^N\\hat\\theta(b_j),$$\n",
    "\n",
    "y la estimación bootstrap de la varianza del estimador será,\n",
    "\n",
    "$$\\hat{Var}_{F_e}(\\hat\\theta) = \\frac1{N-1}\\sum_{j=1}^N(\\hat\\theta(b_j)-\\hat\\theta_m)^2.$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
